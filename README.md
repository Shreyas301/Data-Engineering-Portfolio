# Data-Engineering-Portfolio
My data engineering portfolio showcases skills and projects that demonstrate expertise in designing, building, and maintaining data systems.
## About Me

**Based:** Berlin, Germany

**Current Designation**: Masters student at University of Europe, Potsdam.

**Email:** shreyassandbhor301@gmail.com

**Linkedin**: [My Profile](https://www.linkedin.com/in/shreyas-sandbhor-509a2b213/)

Hello this is Shreyas Sandbhor! Technological evolutions have made human life easier and paved them in a better way. These advancements fascinated and inspired me to pursue a solid master’s degree from an esteemed university in Germany. The use of data science for personalized customer experiences, innovation and risk management has motivated me to build a future in this this field.

I’m currently pursuing Master’s degree in Data Science at University of Europe, Potsdam. I have also done bachelor’s in computer engineering with honours in Artificial Intelligence. Throughout my undergraduate studies I have a grasp of programming languages and in my seventh semester I have been working on databases and its operations as a part of my syllabus, which enabled me to extract necessary information from the data and to apply suitable operations on it for my benefit. Along with practical skills, I have also excelled in my academics with are supported by my high grades in my bachelor’s degree. I have been in top 10% of my batch. Moreover, constant participation in technical events and learning from my seniors has certainly helped me to gain valuable knowledge and deeper insights in this field.



I also got an opportunity to work as an intern for a startup known as R design. I worked there as a Frontend and SQL database engineer. I also managed their cloud services using Microsoft Azure. Along with this internship, I have also freelanced projects based on Machine Learning for students who have completed their bachelors. One of the projects was titled ‘Plant disease Detection using Yolov5’. 

Apart from my academic excellence, I have been very engaged with extra-curricular activities which assisted me to develop leadership, communication skills and ability to work with any group along with producing excellent outcomes. Moreover, I was technical lead at ACM technical club in my university during bachelor's which nourished me with values like team efforts, practical and critical thinking. I have managed many technical events like hackathons and arranged coding sessions for students in their first year of engineering. My research skills and practical knowledge is reflected in my research papers published by me titled “Lane detection system using OpenCV” and “Aircanvas using OpenCV” which are based on Artificial Intelligence and OpenCV.

In future, I want apply my knowledge of data science and contribute to the field. I am pretty convinced that the course that I have obtained will deepen my understanding of computer science and enable me to apply data analysis techniques to real-world problems.


This is a repository in which I can show off my skills, projects, and progression in the early stage of my **Data Engineering / Data Science** career.


### Why specialisation in Data Engineering and Analytics?

What captivates me about data science is its ability to prove its importance in each and every field of technology. I want to dig deeper into the knowledge behind computers and its science to help me create and make advancements in the current technology. I personally think that the combination of AI and data science is second to none. I have a keen interest to improve safety systems for automobiles, I also developed a “lane detection system” using OpenCV and Keras.

## Education

- *University of Europe, Masters in Data Science, Sept 2024
    - Status: **Ongoing** 

- *Savitribai Phule Pune University, India, Bachelors in Computer Engineering, June 2020 - June 2024*
    - Status: **Graduated**

## Certificates

- API Postman Leader
- Python Training - IIT Bombay
- C++ Training - IIt Bombay 

## Portfolio Projects

In my portfolio, you'll find a collection of diverse data engineering and data analysis projects that showcase my skills in extracting valuable insights from data, solving real-world problems, and making data-driven decisions. The below parts is where I placed the files and their details.

### Table of Contents

  - Projects 
    - [Real-time data streaming using Kafka, Airflow, Apache Spark, Cassandra](https://github.com/Shreyas301/Real-time-data-streaming-using-Kafka-Airflow-Apache-Spark-Cassandra)
    - [Real-time stock analysis using Kafka and AWS](https://github.com/Shreyas301/Real-time-stock-analysis-using-kafka-)
    - ELT Pipeline (dbt, Snowflake, Airflow)
    - [Gold trade dashboard between SA and Mali](https://github.com/Shreyas301/Gold-trade-dashboard-between-South-Africa-and-Mali)
    - [EV sales dashboard](https://github.com/Shreyas301/EV-sales-dashboard)
    - [Aircanvas Environment](https://github.com/Shreyas301/Aircanvas-Environment)
    
### 1. Real-time data streaming using Kafka, Airflow, Apache Spark, Cassandra

#### Real-time data streaming using Kafka, Airflow, Apache Spark, Cassandra

**Code**:[Real-time data streaming using Kafka, Airflow, Apache Spark, Cassandra](https://github.com/Shreyas301/Real-time-data-streaming-using-Kafka-Airflow-Apache-Spark-Cassandra)


**Description**: This project involves implementing a real-time data streaming pipeline using Kafka, Apache Airflow, Apache Spark, and Cassandra. The system processes high-velocity data in real-time, enabling near-instantaneous insights. Kafka serves as the messaging queue to handle data ingestion and transport, while Airflow orchestrates and schedules workflows. Apache Spark performs real-time data transformations and analytics, with Cassandra acting as the distributed NoSQL database for low-latency storage and retrieval. The solution is designed for scenarios such as fraud detection, real-time recommendations, or IoT data processing.

**Goal**: The primary goals of this project are to create a fault-tolerant, scalable, and efficient real-time data pipeline. It aims to process data streams with minimal latency, handle high-throughput demands, and ensure system reliability even under heavy workloads. The project also focuses on enabling seamless integration between the technologies and providing a system that can support advanced analytics and business intelligence in real time.

**Skills**: This project requires strong programming skills in Python, Java, or Scala, along with experience in distributed systems and real-time processing. A deep understanding of Kafka's architecture, Airflow workflow orchestration, Spark’s processing capabilities, and Cassandra’s data modeling is essential. Additionally, proficiency in data engineering, stream processing, system monitoring, and troubleshooting is critical for success.

**Technology**: The core technologies used include Apache Kafka for real-time data streaming, Apache Airflow for workflow orchestration, Apache Spark for stream processing and analytics, and Apache Cassandra for distributed database storage. Complementary tools such as Docker for containerization, Kubernetes for orchestration, and monitoring tools like Prometheus or Grafana may be employed to enhance scalability, deployment, and observability.

**Results**: The project successfully delivered a robust, real-time data streaming pipeline capable of processing millions of events per second. It reduced data processing latency to milliseconds, ensured high system availability, and demonstrated scalability to handle future growth. The pipeline provided actionable insights, enabling timely decision-making and achieving significant improvements in operational efficiency and business outcomes.

### 2. Real-time stock analysis using Kafka and AWS

#### Real-time stock analysis using Kafka and AWS

**Code**:[Real-time stock analysis using Kafka and AWS](https://github.com/Shreyas301/Real-time-stock-analysis-using-kafka-)


**Description**: Real-time stock analysis using Kafka involves streaming live stock market data through Kafka topics, where it is processed and analyzed in real time. Kafka serves as a distributed event streaming platform that handles high volumes of stock data, enabling fast and scalable data processing. The data can be consumed by various services or applications that perform analytics, such as calculating stock trends, detecting anomalies, and providing real-time insights. Integrating technologies like Apache Spark, AWS services, or databases helps store and visualize these insights, allowing traders or analysts to make data-driven decisions instantly.

**Goal**: The goal of real-time stock analysis using Kafka is to enable instant processing and analysis of stock market data, providing up-to-the-minute insights into stock performance, trends, and market movements. By leveraging Kafka's event streaming capabilities, the system aims to handle large volumes of data efficiently and ensure low-latency analysis. This empowers traders, analysts, and decision-makers to act on real-time information, detect anomalies, optimize trading strategies, and improve decision-making based on current market conditions.

**Skills**: The skills required for real-time stock analysis using Kafka include expertise in Kafka architecture, data streaming, big data technologies like Apache Spark, cloud computing (AWS), programming in Java, Python, or Scala, data analytics and visualization, database management, and real-time decision-making.

**Technology**: Python, Kafka, Zookeeper, AWS

**Results**: The result of implementing real-time stock analysis using Kafka is a highly efficient and scalable system that processes and analyzes large volumes of stock market data in real time, delivering immediate insights and actionable information. This enables users, such as traders and analysts, to detect market trends, anomalies, and fluctuations as they happen, allowing for faster decision-making, optimized trading strategies, and improved risk management. The system ensures data accuracy and reliability, providing a robust platform for making data-driven decisions in a dynamic market environment.

### 3. ELT Pipeline (dbt, Snowflake, Airflow)
**Code**
**Description**: The project involves building a scalable ELT (Extract, Load, Transform) data pipeline using dbt, Snowflake, and Airflow. The pipeline automates data ingestion, transformation, and orchestration to create a robust and efficient data pipeline for analytics. Data from various sources is loaded into Snowflake, transformed using dbt, and orchestrated with Apache Airflow to ensure smooth execution and monitoring of workflows.
**Goal**: Develop an automated, scalable ELT pipeline to enable reliable, real-time analytics and reporting. Leverage dbt for modular, version-controlled SQL-based transformations. Use Snowflake as the cloud data warehouse for efficient data storage and query performance. Implement Apache Airflow for workflow orchestration to schedule and monitor pipeline tasks. Deliver clean, actionable datasets for business intelligence (BI) tools and decision-making.
**Skills**: Data engineering, SQL development, Workflow autoamation, Cloud Data Warehousing, Version Control
**Technology**: dbt, Snowflake, Apache Airflow, Git, Python, BI Tool.
**Result**: A fully automated and reliable ELT pipeline that delivers clean, transformed data in Snowflake for business intelligence and analytics. Scalable architecture that can handle growing data volumes and new data sources.
Reusable and modular SQL models in dbt ensure better collaboration and maintainability. Airflow ensures smooth task orchestration, error handling, and monitoring. Reduced manual effort and increased data availability for analytics teams.

### 4. Gold trade dashboard between SA and Mali

*Public Repository:* [Python DA](https://github.com/tuanx18/ds-python-projects)

#### Gold trade dashboard between SA and Mali

**Code**:[Gold trade dashboard between SA and Mali](https://github.com/Shreyas301/Gold-trade-dashboard-between-South-Africa-and-Mali)

**Description**: The Gold Trade Dashboard between South Africa (SA) and Mali is a data visualization and analytics platform designed to track, monitor, and analyze gold trade between the two countries. It integrates real-time data on gold exports, imports, pricing trends, volumes, and trade agreements, providing an overview of trade flows, market demand, and pricing fluctuations. The dashboard offers insights that can help policymakers, businesses, and analysts make informed decisions about the gold trade market.

**Goal**: The goal of the Gold Trade Dashboard between South Africa (SA) and Mali is to provide a comprehensive, real-time platform that tracks and visualizes the flow of gold trade between the two countries, enabling stakeholders to make informed decisions, optimize trade strategies, and monitor market trends. The dashboard aims to enhance transparency, improve economic analysis, and facilitate better planning and policy-making for businesses, governments, and traders involved in the gold market.

**Skills**: Key skills for this project include data analysis, data visualization, real-time data integration, understanding of trade economics, and proficiency in business intelligence tools. Additionally, expertise in managing databases, developing interactive dashboards, and integrating APIs for live data feeds is crucial.

**Technology**: Technologies used include data visualization tools like Tableau or Power BI, cloud platforms like AWS for data storage and processing, Python for data analysis and automation, and APIs to gather real-time data from market sources. SQL or NoSQL databases for managing trade data and tools like Apache Kafka for real-time data processing could also be utilized.

**Results**: The result of the Gold Trade Dashboard is an intuitive, interactive platform that enables stakeholders to monitor and analyze the flow of gold between South Africa and Mali, helping to optimize trade strategies, track compliance with trade regulations, and identify trends and opportunities for growth in the gold market. The system enhances transparency, improves decision-making, and provides valuable insights into the economic dynamics between the two countrie

#### 4 EV sales dashboard

**Code** :[EV sales dashboard](https://github.com/Shreyas301/EV-sales-dashboard)

**Description**: The EV (Electric Vehicle) Sales Dashboard is a data analytics platform designed to track and visualize the sales performance of electric vehicles across different regions, models, and time periods. It provides insights into market trends, sales growth, consumer preferences, and regional performance, enabling manufacturers, policymakers, and investors to understand the dynamics of the electric vehicle market. The dashboard presents data in a user-friendly, interactive format, making it easy to analyze sales data, forecasts, and market penetration.

**Goal**: The goal of the EV Sales Dashboard is to provide a comprehensive, real-time view of electric vehicle sales, empowering stakeholders to analyze sales trends, track performance across regions and models, and make data-driven decisions to enhance market strategies and accelerate the adoption of electric vehicles.

**Skills**: data cleaning, data analysis, data visualization, confusion matrix, balancing data, data scaling, machine learning, predictive models

**Technology**: Technologies used include business intelligence tools like Tableau, Power BI, or Google Data Studio, data processing languages like Python or R, cloud services like AWS or Azure for data storage and analysis, and SQL or NoSQL databases to manage sales data. Real-time data integration through APIs and visualization libraries like D3.js or Plotly may also be used for advanced feature

**Results**: The result of the EV Sales Dashboard is an interactive, real-time platform that enables stakeholders to monitor electric vehicle sales performance, identify trends, and make data-driven decisions. It supports manufacturers, dealers, and investors in tracking market penetration, understanding regional preferences, forecasting future sales, and adjusting strategies based on the insights derived from comprehensive sales data. The dashboard ultimately contributes to optimizing marketing efforts, enhancing product offerings, and driving the adoption of electric vehicles.

#### 5 Aircanvas Environment

**Code** :[Aircanvas Environment](https://github.com/Shreyas301/Aircanvas-Environment)

**Description**: Everything which is done by our traditional hardware on our computer is replaced by virtual touch using OpenCV To run the application download and run the whole zip file and run the volumeplus.py on any IDE.

**Goal**: To enable user to control the computer using virtual controls using OpenCV campus.

**Skills**: Skills required include proficiency in computer vision with OpenCV, gesture recognition, real-time image processing, and Python programming. Knowledge of machine learning for gesture classification and touch-free interaction is crucial, as well as experience with user interface design and integrating OpenCV with GUI frameworks to create an interactive tool.

**Technology**: OpenCV, python, Visual Studio code.

**Results**: The result of Aircanvas using OpenCV is an interactive, hands-free tool that allows users to control their computer screen and create content through natural hand movements. It enables a new form of digital interaction, enhancing productivity, creativity, and accessibility by eliminating the need for a mouse or keyboard. This virtual writing tool can be particularly useful in digital art, presentations, education, and other interactive applications.



# THANKS FOR YOUR ATTENTION!

Sincerely,

*Shreyas301*
